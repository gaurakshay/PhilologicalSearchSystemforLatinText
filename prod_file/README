NOTE: The parts below are how I structured my process and not necessarily the how the requirements were structured.

PART 1: FETCH AND STORE

    Here were the problems that I faced while fetching the data:
        1. No two works were structured the same. I personally sometimes felt at loss of ways to try and fetch the data
        because I felt that the site has been structured very poorely. For example, some of the pages have 'pageheader' class
        attributes and others have 'center' and then some have 'h1'. This made automation very difficult. This didn't allow me
        to make a single algorithm to fetch the data. The algorithm was different for every author.
        2. It was difficult for me to understand which tags to use to populate the various attributes of work such as title, book,
        chapter, verse and passage. The reason for the same is again how vastly different each work was presented in the site.
        I tried to try and see a pattern but since their was none, I decided to formulate my own logic to populate the attributes.
        In the end, the author's name and date(for the most part) was available in the home page of the author's work.
        I assumed title of the work to be author's name because most of the times their was not title to be seen.
        I also assumed that each link in a author's home page is a separate book. Upon visiting any book's link the title of
        the resulting page
            was the chapter name. Each paragraph was then assumed to be a passage and verse number was given to each passage.
            Using the above methodology, data was fetched from the below links:
            http://www.thelatinlibrary.com/bacon.html       (1233932 Bytes)
            http://www.thelatinlibrary.com/mirandola.html   (95972 Bytes)
            http://www.thelatinlibrary.com/pauldeacon.html  (557616 Bytes)
            http://www.thelatinlibrary.com/plautus.html     (1772937 Bytes)
            http://www.thelatinlibrary.com/theodosius.html  (1956853 Bytes)        
            http://www.thelatinlibrary.com/williamtyre.html (1977927 Bytes)
            
            The sizes above have been determined by adding up all the "Content-Length" header tag that was returned
            for each request to urls.

            At every stage, I made sure that the "Content-Length" tag in the header tag in response request was equal to the
            actual read values. This helped me make sure that the fetched data is accurate.
            
            The data fetched from the sites was stored in JSON format locally. This was done because first part of the
            requirement was to extract the data only and that JSON is the most widely used way to store structured data 
            such as this.
            
            To replicate the process, please run the following commands to fetch the data and store locally in json format:
            py.test -v bacon.py
            py.test -v mirandola.py
            py.test -v pauldeacon.py
            py.test -v plautus.py
            py.test -v theodosius.py
            py.test -v williamtyre.py
            
            Then, as the second requirement of creating a database, please run the following command:
            py.test -v insertdata.py
            
            To check if the insertion is correct or not, the total number of tuples generated by the data fetched in the first step should be
            equal to the number of rows present in the sqlite3 database.

PART 2: TRANSLATE AND INTERFACE
    
    There were no major hurdles during this phase. I chose php platform as the choice of UI because of the following reasons:
    1. It runs in a browser which can be used by almost everyone.
    2. PHP is secure (which can be useful in the future if security is required. i.e. no security measures were taken in this
        project).
    3. DB connectivity is a breeze in PHP.

    UI:
        The UI is divided in two parts:
        A. TERM SEARCH
            User can enter the search term, notifying whether the term is Latin or English, and then based on the search results,
            a tabulated data is displayed to the user with the snippet of the search and the associated link.

            If the user enters Latin search term, the query is made directly to the db.

            If the user enters English search term, first the MyMemory translation api is invoked, and then all the translations
            of the search term are used to query the database.
        B. USAGE SEARCH
            This is similar to the TERM SEARCH with the only difference being the search query executed against the database.
            I have chosen the name of the author as the reference as it has uses that we read in nltk in the beginning such as
            any author's usage of the particular term.

    NOTE: I have not written separate code for translation. It is integrated into the TERM SEARCH and USAGE SEARCH modules.

    REPLICATION:
        Please make sure to put the two PHP files and the data base in the same directory and it shoud be all good to go.
